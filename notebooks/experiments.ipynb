{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%autoreload 2\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.transforms.v2 as t\n",
    "from lightning import Trainer\n",
    "from segmentation_models_pytorch import Unet\n",
    "\n",
    "import sys; sys.path.append(\"../\") if \"../\" not in sys.path else None\n",
    "from datasets.datamodules import ImageDatasetDataModule \n",
    "from datasets.inria import InriaHDF5 \n",
    "from training.tasks import ClassificationTask\n",
    "from training.utils import (\n",
    "    setup_logger, setup_wandb_logger, setup_checkpoint, #setup_eval\n",
    ")\n",
    "from training.evaluation import EvaluationReport, EvaluationMetricsLogger\n",
    "from training.inference import InferenceCallback\n",
    "from training.attribution import Attribution \n",
    "from etl.pathfactory import PathFactory\n",
    "from etl.etl import reset_dir\n",
    "\n",
    "import os, logging\n",
    "\n",
    "from lightning.pytorch.utilities import disable_possible_user_warnings # type: ignore\n",
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n",
    "disable_possible_user_warnings()\n",
    "\n",
    "os.environ[\"WANDB_CONSOLE\"] = \"off\"\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        \n",
       "Local Dataset: urban_footprint @ [/home/sambhav/datasets/urban_footprint/inria.h5]\n",
       "Configured For: segmentation\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Logging To : /home/sambhav/experiments/urban_footprint_segmentation/testing_new_callbacks\n",
      "WandB Logging To: /home/sambhav/experiments/urban_footprint_segmentation/testing_new_callbacks/wandb\n",
      "Checkpoint Monitoring: val/iou, Checkpoints Saved To: /home/sambhav/experiments/urban_footprint_segmentation/testing_new_callbacks/model_ckpts\n"
     ]
    }
   ],
   "source": [
    "DATASET = InriaHDF5 \n",
    "# MODEL = Unet\n",
    "experiment = {\n",
    "    \"name\": \"testing_new_callbacks\",\n",
    "    \"model_name\": \"unet\",\n",
    "    \"model_params\": {\n",
    "        \"encoder\": \"resnet18\",\n",
    "        \"decoder\": \"deconvolution\",\n",
    "        \"weights\": \"imagenet\",\n",
    "    },\n",
    "\n",
    "    \"dataset_name\": DATASET.NAME,\n",
    "    \"task\": DATASET.TASK,\n",
    "    \"num_classes\": DATASET.NUM_CLASSES,\n",
    "    \"class_names\": DATASET.CLASS_NAMES,\n",
    "\n",
    "    \"random_seed\": 69,\n",
    "\n",
    "    \"test_split\": 0.2,\n",
    "    \"val_split\": 0.1,\n",
    "    \"batch_size\": 2,\n",
    "    \"grad_accum\": 1,\n",
    "    \"num_workers\": 4,\n",
    "\n",
    "    \"loss\": \"binary_cross_entropy\",\n",
    "    \"loss_params\": {\n",
    "        \"reduction\": \"mean\",\n",
    "    },\n",
    "\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"optimizer_params\": {\n",
    "        \"lr\": 1e-5,\n",
    "    },\n",
    "\n",
    "    \"monitor_metric\": \"iou\",\n",
    "    \"monitor_mode\": \"max\",\n",
    "\n",
    "    \"tile_size\": (512, 512),\n",
    "    \"tile_stride\": (512, 512),\n",
    "}\n",
    "PATHS = PathFactory(experiment[\"dataset_name\"], experiment[\"task\"])\n",
    "LOGS_DIR = PATHS.experiments_path / experiment[\"name\"]\n",
    "\n",
    "# NOTE: t.Normalize(DATASET.MEANS, DATASET.STD_DEVS),\n",
    "image_transform = t.Compose([t.ToImage(), t.ToDtype(torch.float32, scale=True)])\n",
    "mask_transform = t.Compose([t.ToImage(), t.ToDtype(torch.float32, scale=False)])\n",
    "#augmentations = t.Compose([t.Pad(6)])\n",
    "augmentations = None\n",
    "\n",
    "datamodule = ImageDatasetDataModule(\n",
    "    root = PATHS.path / \"inria.h5\",\n",
    "    is_remote = False,\n",
    "    is_streaming = False,\n",
    "    dataset_constructor = DATASET, \n",
    "    # dataframe = dataset_df,\n",
    "    image_transform = image_transform,\n",
    "    target_transform = mask_transform,\n",
    "    common_transform = augmentations,\n",
    "    **experiment\n",
    ")\n",
    "model = Unet(experiment[\"model_params\"][\"encoder\"], classes=experiment[\"num_classes\"]) \n",
    "display(datamodule)\n",
    "\n",
    "logger = setup_logger(\n",
    "    logs_dir = PATHS.experiments_path, \n",
    "    name = experiment[\"name\"]\n",
    ")\n",
    "\n",
    "wandb_logger = setup_wandb_logger(\n",
    "    logs_dir = PATHS.experiments_path,\n",
    "    name = experiment[\"name\"]\n",
    ")\n",
    "\n",
    "checkpoint_callback = setup_checkpoint(\n",
    "    ckpt_dir = Path(logger.log_dir, \"model_ckpts\"),\n",
    "    metric = experiment[\"monitor_metric\"],\n",
    "    mode = experiment[\"monitor_mode\"],\n",
    "    save_top_k = \"all\"\n",
    ") \n",
    "\n",
    "# reset_dir(LOGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_CKPT = checkpoint_callback.best_model_path \n",
    "LAST_CKPT = checkpoint_callback.last_model_path\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[checkpoint_callback, EvaluationMetricsLogger()],\n",
    "    #enable_checkpointing=False,\n",
    "    logger = [logger],\n",
    "    enable_model_summary=False,\n",
    "    #fast_dev_run=True,\n",
    "    #num_sanity_val_steps=0,\n",
    "    max_epochs=12,\n",
    "    check_val_every_n_epoch=3, \n",
    "    limit_val_batches=200,\n",
    "    limit_train_batches=200,\n",
    "    limit_test_batches=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# experiment[\"optimizer_params\"][\"lr\"] =  5e-6 \n",
    "trainer.fit(\n",
    "    model=ClassificationTask(model, **experiment),\n",
    "    datamodule=datamodule,\n",
    "    ckpt_path=LAST_CKPT if Path(LAST_CKPT).is_file() else None,\n",
    ")\n",
    "\n",
    "trainer.test(\n",
    "    model=ClassificationTask(model, **experiment),\n",
    "    datamodule=datamodule,\n",
    "    ckpt_path=LAST_CKPT if Path(LAST_CKPT).is_file() else None,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluationReport.plot_experiment_eval_report(\n",
    "    logs_dir=LOGS_DIR,\n",
    "    monitor_metric=experiment[\"monitor_metric\"],\n",
    "    save = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset From: /home/sambhav/experiments/urban_footprint_segmentation/testing_new_callbacks/dataset.csv\n",
      "Loading Samples From: /home/sambhav/experiments/urban_footprint_segmentation/testing_new_callbacks/eval/epoch=11_step=2400_val_samples.csv\n",
      "worst split\n",
      "Returning the worst-25 samples, by IoU\n"
     ]
    }
   ],
   "source": [
    "# LOGS_DIR -> inference -> epoch={epoch}_step={step} -> {split} -> inference_dataset.csv, austin1_0_512_0_512_footprints.geojson, ...\n",
    "# LOGS_DIR -> attribution ->  epoch={epoch}_step={step} -> {filter_by}_{k} -> attribution_dataset.csv, austin1_0_512_0_512_iou={iou}_dice={dice}_gradcam.png, ...\n",
    "epoch = 11 \n",
    "step = 2400 \n",
    "filter_by = \"worst\"\n",
    "k = 25 \n",
    "\n",
    "top_k_df = EvaluationReport.get_top_k_df(\n",
    "    logs_dir = LOGS_DIR,\n",
    "    epoch = epoch,\n",
    "    step = step,\n",
    "    split = \"val\",\n",
    "    filter_by = filter_by,\n",
    "    k = k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = InriaHDF5(\n",
    "    root = PATHS.path / \"inria.h5\",\n",
    "    split = \"val\",\n",
    "    df = top_k_df,\n",
    "    image_transform=image_transform,\n",
    "    target_transform=mask_transform,\n",
    "    common_transform=None,\n",
    "    **experiment,\n",
    ")\n",
    "\n",
    "inference_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset = inference_dataset,\n",
    "    batch_size = experiment[\"batch_size\"] // experiment[\"grad_accum\"],\n",
    "    shuffle = False,\n",
    ")\n",
    "\n",
    "inference_trainer = Trainer(\n",
    "    callbacks=[InferenceCallback(LOGS_DIR, epoch, step)],\n",
    "    #limit_predict_batches=5,\n",
    ")\n",
    "\n",
    "inference_trainer.predict(\n",
    "    model = ClassificationTask(model, **experiment),\n",
    "    dataloaders = inference_dataloader,\n",
    "    ckpt_path = LOGS_DIR / \"model_ckpts\" / f\"epoch={epoch}_step={step}.ckpt\",\n",
    "    return_predictions = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val custom dataset @ [/home/sambhav/datasets/urban_footprint/inria.h5]\n",
      "writing attributions @ [/home/sambhav/experiments/urban_footprint_segmentation/testing_new_callbacks/attribution/epoch=11_step=2400]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7b8c6b16184ad0af46a7d424287d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving Attribution Maps:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attribution_dataset = DATASET(\n",
    "    root = PATHS.path / \"inria.h5\",\n",
    "    split = \"val\",\n",
    "    df = top_k_df,\n",
    "    image_transform=image_transform,\n",
    "    target_transform=mask_transform,\n",
    ")\n",
    "\n",
    "Attribution(LOGS_DIR, 11, 2400, model, attribution_dataset).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
